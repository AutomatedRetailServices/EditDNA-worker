import os
import uuid
import tempfile
import traceback
from typing import List, Dict, Any

from .asr_adapter import transcribe_with_whisper
from .semantic_visual_pass import semantic_visual_pass
from .ffmpeg_utils import concat_clips_ffmpeg


def run_pipeline(
    session_id: str,
    file_urls: List[str],
    portrait: bool,
    funnel_counts: str,
    max_duration: int,
    bin_sec: float,
    min_take_sec: float,
    max_take_sec: float,
    veto_min_score: float,
    sem_merge_sim: float,
    viz_merge_sim: float,
    merge_max_chain: int,
    filler_tokens: List[str],
    filler_max_rate: float,
    micro_cut: bool,
    micro_silence_db: float,
    micro_silence_min: float,
    slot_require_product: List[str],
    slot_require_ocr_cta: str,
    fallback_min_sec: int,
) -> Dict[str, Any]:
    """
    Core EditDNA Stage-7 pipeline:
      1. Download input video
      2. ASR transcription (with Sentence-Boundary pass)
      3. Semantic classification + micro cuts
      4. Funnel assembly (HOOK->PROBLEM->FEATURE->PROOF->CTA)
      5. FFmpeg concat export
    Returns a dict consumed by tasks.job_render()
    """

    tlocal = tempfile.gettempdir()
    input_local = os.path.join(tlocal, f"in_{uuid.uuid4().hex}.mp4")
    final_local = os.path.join(tlocal, f"out_{uuid.uuid4().hex}.mp4")

    try:
        # ------------------------------------------------------------------
        # 1️⃣  Download input
        # ------------------------------------------------------------------
        from .download_utils import download_video
        first_url = file_urls[0]
        download_video(first_url, input_local)

        # ------------------------------------------------------------------
        # 2️⃣  ASR  (Whisper + Sentence-Boundary)
        # ------------------------------------------------------------------
        asr_enabled = bool(int(os.getenv("ASR_ENABLED", "1")))
        transcript = []
        if asr_enabled:
            transcript = transcribe_with_whisper(
                input_path=input_local,
                model_size=os.getenv("ASR_MODEL", "small"),
                bin_sec=bin_sec,
                min_take_sec=min_take_sec,
                max_take_sec=max_take_sec,
            )

        # ------------------------------------------------------------------
        # 3️⃣  Semantic + Visual pass
        # ------------------------------------------------------------------
        semantic_output = semantic_visual_pass(
            transcript=transcript,
            funnel_counts=funnel_counts,
            veto_min_score=veto_min_score,
            filler_tokens=filler_tokens,
            filler_max_rate=filler_max_rate,
            sem_merge_sim=sem_merge_sim,
            viz_merge_sim=viz_merge_sim,
            merge_max_chain=merge_max_chain,
            micro_cut=micro_cut,
            micro_silence_db=micro_silence_db,
            micro_silence_min=micro_silence_min,
        )

        clips = semantic_output.get("clips", [])
        slots = semantic_output.get("slots", {})

        if not clips:
            raise RuntimeError("No valid clips generated by semantic pass")

        # ------------------------------------------------------------------
        # 4️⃣  Assemble final funnel video
        # ------------------------------------------------------------------
        concat_clips_ffmpeg(clips, input_local, final_local, portrait)

        # ------------------------------------------------------------------
        # ✅ Return standard structure
        # ------------------------------------------------------------------
        return {
            "ok": True,
            "input_local": input_local,
            "final_local": final_local,
            "duration_sec": float(os.getenv("DEFAULT_DURATION", "0")),
            "clips": clips,
            "slots": slots,
        }

    except Exception as e:
        return {
            "ok": False,
            "error": f"Pipeline error: {repr(e)}",
            "trace": traceback.format_exc(),
        }
